{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d81d08",
   "metadata": {},
   "source": [
    "# Linear Regression: Least Squares and Gradient Descent (California Housing Dataset)\n",
    "\n",
    "- Author: Ibrahim Radwan\n",
    "- Date: 27 November 2024\n",
    "- Course: Supervised Machine Learning, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e48e5",
   "metadata": {},
   "source": [
    "\n",
    "### Objective\n",
    "This notebook demonstrates two approaches to solving linear regression problems using the **California Housing Dataset**:\n",
    "\n",
    "1. **Least Squares** method: Solves the regression problem analytically using the normal equation.\n",
    "2. **Gradient Descent** method: Iteratively optimizes the model parameters by minimizing the mean squared error.\n",
    "\n",
    "Both methods include data preparation, model fitting, and evaluation of Mean Squared Error (MSE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0737b",
   "metadata": {},
   "source": [
    "\n",
    "### 1- Least Squares Method\n",
    "\n",
    "The least squares method solves the regression problem using the normal equation:\n",
    "\n",
    "\\[ \\theta = (X^T X)^{-1} X^T y \\]\n",
    "\n",
    "This approach is efficient for small to medium-sized datasets but can be computationally expensive for larger datasets due to the computation needed for the matrix inversion operation.\n",
    "\n",
    "Below, we apply this method to the California Housing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d0832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-3.70232777e+01  4.48674910e-01  9.72425752e-03 -1.23323343e-01\n",
      "  7.83144907e-01 -2.02962058e-06 -3.52631849e-03 -4.19792487e-01\n",
      " -4.33708065e-01]\n",
      "Mean Squared Error on Test Set: 0.56\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California housing dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "X = california.data # Independent variables (Features)\n",
    "y = california.target # Dependent variable (Target)\n",
    "\n",
    "# Add intercept term (This step is needed to facilitate the normal equation solution, where intercept becomes part of the parameters)\n",
    "X['Intercept'] = 1\n",
    "X = X[['Intercept'] + list(X.columns[:-1])]  # Place intercept first\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train\n",
    "\n",
    "# Least squares solution (\\[ \\theta = (X^T X)^{-1} X^T y \\])\n",
    "coefficients = np.linalg.inv(X_train_np.T @ X_train_np) @ X_train_np.T @ y_train_np\n",
    "\n",
    "# Predictions on test set (\\[ \\y^{hat} = (X \\theta])\n",
    "y_test_pred = X_test.values @ coefficients\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Coefficients: {coefficients}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d9e6b",
   "metadata": {},
   "source": [
    "\n",
    "### 2- Gradient Descent Method\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that minimizes the cost function:\n",
    "\n",
    "\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\n",
    "\n",
    "At each step, the parameters are updated using:\n",
    "\n",
    "\\[ \\theta := \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta) \\]\n",
    "\n",
    "where \\( \\alpha \\) is the learning rate.\n",
    "\n",
    "Below, we implement gradient descent to optimize the model parameters for the California Housing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fbb581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, MSE: 5.6297\n",
      "Iteration 100, MSE: 1.2989\n",
      "Iteration 200, MSE: 0.7100\n",
      "Iteration 300, MSE: 0.6183\n",
      "Iteration 400, MSE: 0.5953\n",
      "Iteration 500, MSE: 0.5832\n",
      "Iteration 600, MSE: 0.5739\n",
      "Iteration 700, MSE: 0.5661\n",
      "Iteration 800, MSE: 0.5594\n",
      "Iteration 900, MSE: 0.5537\n",
      "\n",
      "Final Coefficients:\n",
      "[ 2.06811463  0.82208184  0.17853056 -0.1289977   0.1560443   0.01671756\n",
      " -0.04046974 -0.48787047 -0.45130532]\n",
      "Mean Squared Error on Test Set: 0.5673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load California housing dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "# Add intercept term\n",
    "X['Intercept'] = 1\n",
    "X = X[['Intercept'] + list(X.columns[:-1])]  # Place intercept first\n",
    "\n",
    "# Standardize features (excluding intercept)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled.iloc[:, 1:] = scaler.fit_transform(X.iloc[:, 1:])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test\n",
    "\n",
    "# Gradient Descent Parameters\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "m, n = X_train_np.shape  # m: number of examples, n: number of features\n",
    "theta = np.zeros(n)  # Initialize weights\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for i in range(iterations):\n",
    "    # Predictions\n",
    "    y_pred = X_train_np @ theta\n",
    "    # Compute the gradient\n",
    "    gradient = (1 / m) * (X_train_np.T @ (y_pred - y_train_np))\n",
    "    # Update the parameters\n",
    "    theta -= learning_rate * gradient\n",
    "    \n",
    "    # Optionally print the loss every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        mse = mean_squared_error(y_train_np, y_pred)\n",
    "        print(f\"Iteration {i}, MSE: {mse:.4f}\")\n",
    "\n",
    "# Test set predictions\n",
    "y_test_pred = X_test_np @ theta\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse_test = mean_squared_error(y_test_np, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal Coefficients:\")\n",
    "print(theta)\n",
    "print(f\"Mean Squared Error on Test Set: {mse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0dd68",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "\n",
    "- Can you spot a difference in the results between the two techniques (Results, and computational power)?\n",
    "- What will happen if we changed the number of iterations in the Gradient Descent approach? \n",
    "- Can you visualize the predictions for both approaches?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
